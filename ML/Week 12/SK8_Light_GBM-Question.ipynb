{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ed39d78-31c8-4df7-86d9-049eab667b12",
   "metadata": {},
   "source": [
    "# Light GBM & Parameter Tuning with Optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28bd9e2f-3efa-40fc-8320-e889dc74f6c2",
   "metadata": {},
   "source": [
    "A critical step in machine learning is to identify the \"best\" hyper-parameter values for a model (such as the number of neighbours in a KNN model). In this tutorial, we illustrate how a good set of model hyper-parameters can be found within a cross-validation framework. The particular family of models we focus on is the Light GBM model, which is a high-performing popular ensemble method with several attractive features as discussed below. For tuning, we use the Optuna hyper-parameter tuning module due to its flexibility and good performance in general. For illustration of these concepts, we use the US Census Income Dataset for a binary classification problem.\n",
    "\n",
    "In what follows, we first provide an overview of boosting, gradient boosting, and Light GBM. Then we illustrate how Light GBM's hyper-parameters can be fine-tuned by Optuna using the\n",
    "US Census Income Dataset. We also discuss how the modelling & tuning results can be visualised."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff2eda3-730f-4c31-b569-67578f4be12f",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Boosting, Gradient Boosting, and Light GBM](#lgbm)\n",
    "- [Case Study & Light GBM Features](#case_study)\n",
    "- [Hyper-parameter Tuning with Optuna](#optuna)\n",
    "- [Evaluation & Visualisation of Results](#vis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eeb4b84-8458-4ca2-a9b5-a7535bec9c8e",
   "metadata": {},
   "source": [
    "## Boosting, Gradient Boosting, and Light GBM  <a id='lgbm'></a>\n",
    "\n",
    "**Reference:** *Fundamentals of Machine Learning for Predictive Data Analytics Algorithms, Worked Examples, and Case Studies* By John D. Kelleher, Brian Mac Namee and Aoife Dâ€™Arcy.\n",
    "\n",
    "\n",
    "### Boosting\n",
    "\n",
    "Boosting works by iteratively creating models and adding them to an ensemble, usually consisting of decision trees. The iteration stops when a predefined number of models have been added. In classical boosting such as ADABoost, each new model added to the ensemble is biased to pay more attention to instances that previous models misclassified. This is done by incrementally adapting the dataset used to train the models. To do this, we use a weighted dataset. Each instance has an associated weight $w_i \\geq 0$, initially set to $1/n$ where n is the number of instances in the dataset. After each model is added to the ensemble, it is tested on the training data and the weights of the instances the model gets correct are decreased and the weights of the instances the model gets incorrect are increased. These weights are used as a distribution over which the dataset is sampled to created a replicated training set, where the replication of an instance is proportional to its weight. Once the set of models have been created, the ensemble makes predictions using a weighted aggregate of the predictions made by the individual models. The weights used in this aggregation are simply the confidence factors associated with each model.\n",
    "\n",
    "\n",
    "### Gradient Boosting\n",
    "\n",
    "Like simpler boosting algorithms, gradient boosting iteratively trains prediction models in an attempt to make later models specialize in areas that earlier models struggled with. Gradient boosting can be said to do so in a more aggressive way than the boosting algorithm described previously. In gradient boosting, later models are trained to **directly correct errors made by earlier models**, rather than the more subtle approach of simply changing weights in a sampling distribution. \n",
    "\n",
    "Although any model can be used at the model training steps in gradient boosting, it is most common to use relatively shallow decision trees. This is the same approach taken in random forests, which aims to combine a large number of weak learners into an overall strong learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4222ebf6-11e2-4944-bab9-6344e7f3de67",
   "metadata": {},
   "source": [
    "### Light GBM Overview\n",
    "\n",
    "Light GBM (Light Gradient Boosting Machine) is popular open-source tree-based boosting method developed and maintained by Microsoft. The 2017 NIPS Conference proceeding introducing this algorithm can be found [here](https://proceedings.neurips.cc/paper/2017/file/6449f44a102fde848669bdd9eb6b76fa-Paper.pdf). Light GBM uses the leaf-wise tree growth algorithm, while many other popular methods such as Random Forests and XGBoost use depth-wise tree growth. Compared with depth-wise growth, the leaf-wise algorithm can converge much faster. \n",
    "\n",
    "Light GBM has many of XGBoost's advantages, including sparse optimization, parallel training, multiple loss functions, regularization, bagging, and early stopping. A major difference between the two lies in the construction of trees. Light GBM does not grow a tree level-wise (row by row) as most other implementations do. Instead, it grows trees leaf-wise. It chooses the leaf it believes will yield the largest decrease in loss. Besides, Light GBM does not use the widely-used sorted-based decision tree learning algorithm, which searches the best split point on sorted feature values, as XGBoost or other implementations do. Instead, Light GBM implements a highly optimized histogram-based decision tree learning algorithm, which yields great advantages on both efficiency and memory consumption ([source 1](https://en.wikipedia.org/wiki/LightGBM), [source 2](https://towardsdatascience.com/how-to-beat-the-heck-out-of-xgboost-with-lightgbm-comprehensive-tutorial-5eba52195997)).\n",
    "\n",
    "In a full-scale Light GBM vs. XGBoost comparison regarding income status prediction, it was observed that Light GBM was about 25 times faster than XGBoost while resulting in about the same model performance with respect to an f1-score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd6f4cd-789c-4837-8fdc-cce76d6c45fb",
   "metadata": {},
   "source": [
    "## Case Study: Income Status Prediction & Light GBM Features <a id='case_study'></a>\n",
    "\n",
    "### Getting Started\n",
    "\n",
    "In this tutorial, we will work with the US Census Income Dataset from [UCI](https://archive.ics.uci.edu/ml/datasets/census+income). The data was taken from the 1994 US Census Database. The goal is to predict whether an individual earns more than $50,000 a year. We read in a cleaned version of this dataset with no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce18a52d-f2ed-4eb4-a6ff-8617d825acfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.max_rows', None) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "930986f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.24.0\n"
     ]
    }
   ],
   "source": [
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3318e991-9c66-4d80-a68b-76c9b63d4545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "Index(['age', 'workclass', 'education_num', 'marital_status', 'occupation',\n",
      "       'relationship', 'race', 'gender', 'hours_per_week', 'native_country',\n",
      "       'capital', 'income_status'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "# how to read a csv file from a github account\n",
    "url_name = 'https://raw.githubusercontent.com/akmand/datasets/master/us_census_income_data_clean.csv'\n",
    "url_content = requests.get(url_name, verify=False).content\n",
    "\n",
    "df_raw = pd.read_csv(io.StringIO(url_content.decode('utf-8')))\n",
    "\n",
    "df_raw = df_raw.rename(columns={'result': 'target'})\n",
    "\n",
    "print(df_raw.shape)\n",
    "print(df_raw.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b15be670-f97e-403f-ab8c-265d2b6735e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>workclass</th>\n",
       "      <th>education_num</th>\n",
       "      <th>marital_status</th>\n",
       "      <th>occupation</th>\n",
       "      <th>relationship</th>\n",
       "      <th>race</th>\n",
       "      <th>gender</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>native_country</th>\n",
       "      <th>capital</th>\n",
       "      <th>income_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>state_gov</td>\n",
       "      <td>13</td>\n",
       "      <td>never_married</td>\n",
       "      <td>adm_clerical</td>\n",
       "      <td>not_in_family</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>40</td>\n",
       "      <td>united_states</td>\n",
       "      <td>2174</td>\n",
       "      <td>&lt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50</td>\n",
       "      <td>self_emp_not_inc</td>\n",
       "      <td>13</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>exec_managerial</td>\n",
       "      <td>husband</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>13</td>\n",
       "      <td>united_states</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38</td>\n",
       "      <td>private</td>\n",
       "      <td>9</td>\n",
       "      <td>divorced</td>\n",
       "      <td>handlers_cleaners</td>\n",
       "      <td>not_in_family</td>\n",
       "      <td>white</td>\n",
       "      <td>male</td>\n",
       "      <td>40</td>\n",
       "      <td>united_states</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>53</td>\n",
       "      <td>private</td>\n",
       "      <td>7</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>handlers_cleaners</td>\n",
       "      <td>husband</td>\n",
       "      <td>other</td>\n",
       "      <td>male</td>\n",
       "      <td>40</td>\n",
       "      <td>united_states</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>private</td>\n",
       "      <td>13</td>\n",
       "      <td>married_civ_spouse</td>\n",
       "      <td>prof_specialty</td>\n",
       "      <td>wife</td>\n",
       "      <td>other</td>\n",
       "      <td>female</td>\n",
       "      <td>40</td>\n",
       "      <td>other</td>\n",
       "      <td>0</td>\n",
       "      <td>&lt;=50k</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   age         workclass  education_num      marital_status  \\\n",
       "0   39         state_gov             13       never_married   \n",
       "1   50  self_emp_not_inc             13  married_civ_spouse   \n",
       "2   38           private              9            divorced   \n",
       "3   53           private              7  married_civ_spouse   \n",
       "4   28           private             13  married_civ_spouse   \n",
       "\n",
       "          occupation   relationship   race  gender  hours_per_week  \\\n",
       "0       adm_clerical  not_in_family  white    male              40   \n",
       "1    exec_managerial        husband  white    male              13   \n",
       "2  handlers_cleaners  not_in_family  white    male              40   \n",
       "3  handlers_cleaners        husband  other    male              40   \n",
       "4     prof_specialty           wife  other  female              40   \n",
       "\n",
       "  native_country  capital income_status  \n",
       "0  united_states     2174         <=50k  \n",
       "1  united_states        0         <=50k  \n",
       "2  united_states        0         <=50k  \n",
       "3  united_states        0         <=50k  \n",
       "4          other        0         <=50k  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df_raw.copy()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3313606-780f-4dd2-b8af-c828af9991c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<=50k    34014\n",
       ">50k     11208\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's have a look at the distribution of the target feature\n",
    "df = df.rename(columns={'income_status': 'target'})\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3ac08c8e-605a-45d3-9114-8656c2602568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    34014\n",
       "1    11208\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we need to map the target feature so that \n",
    "# the positive class is 1 and the negative class is 0\n",
    "df['target'] = np.where(df['target'] == \"<=50k\", 0, 1)\n",
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e576360-b735-4bfb-b088-6987a7c441c5",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d306e66a-6ed5-4541-8628-11f236f0377c",
   "metadata": {},
   "source": [
    "Here, we need to make sure that we run the column filters **after** row filters, if any. Please note that the steps listed below are mostly generic. Let's first order the columns alphabetically for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "edb6c452-5598-4392-b9fa-ce51e433cdf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[sorted(df.columns)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c235f9d0-8631-4e28-b2da-67e11fe902ec",
   "metadata": {},
   "source": [
    "### Dropping Columns with a Single Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee2c9148-f0aa-4ceb-b594-bf6f6bbd1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "columns dropped:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "unique_value_cols = [col for col in df.columns if df[col].nunique(dropna=True) <= 1]\n",
    "df = df.drop(columns=unique_value_cols)\n",
    "print(df.shape)\n",
    "print('columns dropped:')\n",
    "print(unique_value_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8924f-2a69-419e-8286-f8d55d088df2",
   "metadata": {},
   "source": [
    "### Dropping Columns with Too Many Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7f81a6d-14c6-422c-bd74-f2580d980b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "columns dropped:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# run this BEFORE the \"too frequent\" filter below to avoid undefined mode\n",
    "cols_before_na_filter = df.columns\n",
    "column_min_allowed_nonmissing_ratio = 0.50\n",
    "#\n",
    "df = df.dropna(axis=1, thresh=int(column_min_allowed_nonmissing_ratio*df.shape[0] + 1))\n",
    "#\n",
    "print(df.shape)\n",
    "print('columns dropped:')\n",
    "print(sorted(list(set(cols_before_na_filter) - set(df.columns))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913fa0a0-da72-4546-87f4-16e44cbecddc",
   "metadata": {},
   "source": [
    "### Dropping Columns with Too Frequent Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5606d87a-21e3-45e0-ad32-c34118f9bcac",
   "metadata": {},
   "source": [
    "If a column's most frequent value is \"too frequent\" and there are enough number of levels, let's drop that column as it will probably not be very helpful for predictive modelling. Below, `eq()` checks for equality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8011ec2c-2a7b-47d6-af08-cb308a07970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unhelpful_cols(df, min_levels_threshold, freq_threshold):\n",
    "    # get all columns that are objects\n",
    "    # these are assumed to be **nominal** categorical\n",
    "    Data = df.drop(columns = 'target')\n",
    "    categorical_cols = Data.columns[Data.dtypes == object].tolist()\n",
    "    return [col for col in categorical_cols if (df[col].nunique() >= min_levels_threshold) and \n",
    "            (df[col].eq(df[col].mode(dropna=True)[0]).mean() >= freq_threshold)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63693402-46f0-4804-87d7-98ed8e1a32ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "columns dropped:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# if there are at least 5 levels in a column and the most frequent level is at least 95% of the data, drop it\n",
    "unhelpful_cols = get_unhelpful_cols(df, 5, 0.95)\n",
    "#\n",
    "df = df.drop(columns=unhelpful_cols)\n",
    "print(df.shape)\n",
    "print('columns dropped:')\n",
    "print(unhelpful_cols)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab8069d9-850c-4495-9846-f8d15137b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "columns dropped:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# if there are at least 2 levels in a column and the most frequent level is at least 99% of the data, drop it\n",
    "unhelpful_cols = get_unhelpful_cols(df, 2, 0.99)\n",
    "#\n",
    "df = df.drop(columns=unhelpful_cols)\n",
    "print(df.shape)\n",
    "print('columns dropped:')\n",
    "print(unhelpful_cols)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7e68ff-01dd-4937-83dc-999146e7192c",
   "metadata": {},
   "source": [
    "### Dropping Columns with Too Many Levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6206d1d-262e-4886-91bb-b8d118afe3d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "columns dropped:\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# if a column has too many levels, drop it\n",
    "Data = df.drop(columns = 'target')\n",
    "categorical_cols = Data.columns[Data.dtypes == object].tolist()\n",
    "#\n",
    "col_unique_value_threshold = 150\n",
    "#\n",
    "unhelpful_cols = [col for col in categorical_cols if (df[col].nunique() >= col_unique_value_threshold)]\n",
    "df = df.drop(columns=unhelpful_cols)\n",
    "print(df.shape)\n",
    "print('columns dropped:')\n",
    "print(unhelpful_cols)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2de9b8f1-b1b4-47c7-9180-d0ecff36fa25",
   "metadata": {},
   "source": [
    "### Dropping Identical Columns\n",
    "\n",
    "For datasets coming from database joins, sometimes there will be columns with different names that are essentially the same. We can use the value distributions to check for such columns and then keep only one of them if they turn out to be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac9b7d6-292d-4887-9737-437be9b8394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_identical_value_count_cols(df):\n",
    "    return [(df.columns[col1], df.columns[col2]) for col1 in range(len(df.columns)) for col2 in range(col1+1, len(df.columns)) \n",
    "        if np.array_equal(df.iloc[:, col1].value_counts().values, df.iloc[:,col2].value_counts().values)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e858488-5071-46ae-bd96-a2023fb2c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['foo'] = df['marital_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e5c9977-1f42-496a-98dd-60efa723a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('marital_status', 'foo')]\n"
     ]
    }
   ],
   "source": [
    "identical_value_cols = get_identical_value_count_cols(df)\n",
    "print(identical_value_cols)\n",
    "# placeholder:\n",
    "identical_value_cols_to_drop = ['foo']\n",
    "df = df.drop(columns=identical_value_cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ec4c97-d838-4648-8dd9-7c6d460a75bb",
   "metadata": {},
   "source": [
    "### Missing Values and Light GBM\n",
    "\n",
    "A nice feature of Light GBM is that all features are discretized & binned behind the scenes, so Light GBM can **handle missing values natively**. However, for this to happen, missing values need to be set to `np.nan`.  This dataset has no missing values, but still, we can present the missing value results in a nice-looking HTML table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c10d5e87-9ded-4725-941e-6d3e40096504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing value counts:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('missing value counts:')\n",
    "\n",
    "cols_missing_values = df.isna().sum()\n",
    "cols_missing_values[cols_missing_values > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2546064e-c5ec-4603-a34f-3b4d0fb59217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape before dropping rows with missing values: (45222, 12)\n",
      "shape after: (45222, 12)\n"
     ]
    }
   ],
   "source": [
    "print('shape before dropping rows with missing values:', df.shape)\n",
    "df = df.dropna()\n",
    "print('shape after:', df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac7194c-afab-48f0-a990-c65c816732e3",
   "metadata": {},
   "source": [
    "### Dropping Duplicate Rows\n",
    "\n",
    "We need to drop duplicate rows at the very end in case some columns were dropped, potentially resulting in even more duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c67f4439-15c3-4961-aedd-2e1538f6eec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45222, 12)\n",
      "(39123, 12)\n"
     ]
    }
   ],
   "source": [
    "# drop duplicate WOs\n",
    "print(df.shape)\n",
    "df = df.drop_duplicates(keep=\"first\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbacbf08-4f3d-4486-a693-8c3fa7272447",
   "metadata": {},
   "source": [
    "### Feature Analysis - Numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9cf5aaff-a9f2-47e7-9317-4feca752e767",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>capital</th>\n",
       "      <th>education_num</th>\n",
       "      <th>hours_per_week</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>39123.00</td>\n",
       "      <td>39123.00</td>\n",
       "      <td>39123.00</td>\n",
       "      <td>39123.00</td>\n",
       "      <td>39123.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.33</td>\n",
       "      <td>1155.50</td>\n",
       "      <td>10.14</td>\n",
       "      <td>41.21</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>13.30</td>\n",
       "      <td>8033.21</td>\n",
       "      <td>2.64</td>\n",
       "      <td>12.47</td>\n",
       "      <td>0.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>17.00</td>\n",
       "      <td>-4356.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>29.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>9.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>38.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>10.00</td>\n",
       "      <td>40.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>13.00</td>\n",
       "      <td>45.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>90.00</td>\n",
       "      <td>99999.00</td>\n",
       "      <td>16.00</td>\n",
       "      <td>99.00</td>\n",
       "      <td>1.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           age  capital  education_num  hours_per_week   target\n",
       "count 39123.00 39123.00       39123.00        39123.00 39123.00\n",
       "mean     39.33  1155.50          10.14           41.21     0.25\n",
       "std      13.30  8033.21           2.64           12.47     0.44\n",
       "min      17.00 -4356.00           1.00            1.00     0.00\n",
       "25%      29.00     0.00           9.00           40.00     0.00\n",
       "50%      38.00     0.00          10.00           40.00     0.00\n",
       "75%      48.00     0.00          13.00           45.00     1.00\n",
       "max      90.00 99999.00          16.00           99.00     1.00"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.columns[df.dtypes != object]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281f6df1-f182-4165-957d-6cd2f81a620d",
   "metadata": {},
   "source": [
    "### Feature Analysis - Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0bbccae8-970a-4a09-a94a-bc1438fcfa77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_cat_col_values(df, print_levels=True, print_perc=False, dont_print_more_than=20):\n",
    "    \n",
    "    cat_cols = df.columns[df.dtypes==object].tolist()\n",
    "    \n",
    "    if print_levels:\n",
    "        if print_perc:\n",
    "            print('printing percentage counts:\\n')\n",
    "        else:\n",
    "            print('printing absolute counts:\\n')\n",
    "        \n",
    "    for col in cat_cols:\n",
    "        print(f'{col}: {df[col].nunique()} unique values')\n",
    "        if print_levels:\n",
    "            if print_perc:\n",
    "                print(df[col].value_counts(normalize=True)[:dont_print_more_than]*100)\n",
    "            else:\n",
    "                print(df[col].value_counts()[:dont_print_more_than])\n",
    "        print('-------')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "64841467-57cb-4076-8972-faa0b5dd6f47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing percentage counts:\n",
      "\n",
      "gender: 2 unique values\n",
      "male     66.47\n",
      "female   33.53\n",
      "Name: gender, dtype: float64\n",
      "-------\n",
      "marital_status: 7 unique values\n",
      "married_civ_spouse      45.71\n",
      "never_married           30.98\n",
      "divorced                15.03\n",
      "separated                3.56\n",
      "widowed                  3.22\n",
      "married_spouse_absent    1.41\n",
      "married_af_spouse        0.08\n",
      "Name: marital_status, dtype: float64\n",
      "-------\n",
      "native_country: 2 unique values\n",
      "united_states   90.18\n",
      "other            9.82\n",
      "Name: native_country, dtype: float64\n",
      "-------\n",
      "occupation: 14 unique values\n",
      "prof_specialty      14.08\n",
      "exec_managerial     13.55\n",
      "adm_clerical        12.07\n",
      "sales               11.91\n",
      "craft_repair        11.77\n",
      "other_service       10.84\n",
      "machine_op_inspct    6.20\n",
      "transport_moving     5.19\n",
      "handlers_cleaners    4.35\n",
      "farming_fishing      3.64\n",
      "tech_support         3.38\n",
      "protective_serv      2.39\n",
      "priv_house_serv      0.59\n",
      "armed_forces         0.04\n",
      "Name: occupation, dtype: float64\n",
      "-------\n",
      "race: 2 unique values\n",
      "white   84.41\n",
      "other   15.59\n",
      "Name: race, dtype: float64\n",
      "-------\n",
      "relationship: 6 unique values\n",
      "husband          39.83\n",
      "not_in_family    26.87\n",
      "own_child        13.20\n",
      "unmarried        11.61\n",
      "wife              5.12\n",
      "other_relative    3.37\n",
      "Name: relationship, dtype: float64\n",
      "-------\n",
      "workclass: 7 unique values\n",
      "private            70.56\n",
      "self_emp_not_inc    9.37\n",
      "local_gov           7.59\n",
      "state_gov           4.84\n",
      "self_emp_inc        4.08\n",
      "federal_gov         3.50\n",
      "without_pay         0.05\n",
      "Name: workclass, dtype: float64\n",
      "-------\n"
     ]
    }
   ],
   "source": [
    "print_cat_col_values(df, print_levels=True, print_perc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd7e55a-6e05-4345-9940-19160ddd3f49",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining Descriptive and Target Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9dd99a3a-3281-4d6b-8e45-5720e82263cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = df.drop(columns = 'target').copy()\n",
    "target = np.array(df['target']).reshape(-1, 1)\n",
    "\n",
    "# get all columns that are objects\n",
    "# these are assumed to be **nominal** categorical\n",
    "categorical_cols = Data.columns[Data.dtypes == object].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa88643-3b10-400b-8255-346250f3b379",
   "metadata": {},
   "source": [
    "With Light GBM, we do **not** need One-Hot-Encoding (OHE) of categorical features. **HOWEVER**, we must set their data type to `category` for the algorithm to work properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a4554e17-9530-4c7e-900c-5604c324740e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                  int64\n",
       "capital              int64\n",
       "education_num        int64\n",
       "gender            category\n",
       "hours_per_week       int64\n",
       "marital_status    category\n",
       "native_country    category\n",
       "occupation        category\n",
       "race              category\n",
       "relationship      category\n",
       "workclass         category\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Data[categorical_cols] = Data[categorical_cols].astype('category')\n",
    "\n",
    "# let's verify the results\n",
    "Data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4163a2a4-be4f-4cdb-b36e-359bbf1bce77",
   "metadata": {},
   "source": [
    "### Handling of Categorical Features in Light GBM\n",
    "\n",
    "*Nominal* categorical features have no natural ordering. Some examples are colours, make of cars, country names, etc. This is unlike *ordinal* categorical features such as credit ratings and course grades that do have a natural, inherent ordering.\n",
    "\n",
    "In general, nominal categorical features need to be one-hot-encoded before fitting any prediction models. Light GBM is an exception to this rule as it can **handle (nominal) categorical features natively** without any encoding! **HOWEVER**, for this functionality to work, data type of nominal categorical features need to be set to Pandas' `category` data type. That is, a categorical feature whose data type is set to `category` will be treated as *nominal* categorical by Light GBM. \n",
    "\n",
    "On the other hand, ordinal categorical features whose values are strings need to be mapped correctly as integers, such as below:\n",
    "- \"Good\" credit rating --> 3\n",
    "- \"OK\" credit rating --> 2\n",
    "- \"Bad\" credit rating --> 1\n",
    "\n",
    "Once this mapping is done, Light GBM will handle them correctly (just like any other numerical feature)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f6aca-ebaa-4b48-9a04-a92c91ef1de1",
   "metadata": {},
   "source": [
    "### Under the Hood for Categorical Features\n",
    "\n",
    "As mentioned above, Light GBM can handle nominal categorical features natively once their data type is set to `category`. Internally, Light GBM splits categorical features into just two groups while growing a boosting tree ([source](https://lightgbm.readthedocs.io/en/latest/Features.html#optimal-split-for-categorical-features)). In order to find the optimal split, it considers only a small number of splits controlled by the following parameters:\n",
    "- `min_data_per_group`: (default: 100) Minimum number of observations withing a group.\n",
    "- `max_cat_threshold`: (default: 32) Maximum number of split points considered during a (binary) categorical feature split.\n",
    "- `max_cat_to_onehot`: (default: 4) By default, if a feature has 4 or less number of levels, Light GBM will split it into 2 by the best of all the one-vs-all_other_levels split candidates.\n",
    "\n",
    "The main idea with splitting a categorical feature is to sort the categories according to the training objective at each split. Light GBM sorts this feature's histogram based on its accumulated values and then finds the best split on the sorted histogram.\n",
    "\n",
    "\n",
    "### Handling of Numerical Features\n",
    "Light GBM discretizes all numerical features behind the scenes for the binary splits. This binning is based on these numerical features' histograms. Number of bins is controlled by the parameters below:\n",
    "- `max_bin`: (default: 255) Maximum number of bins the numerical features will be bucketed in. Smaller number of bins might help with speed as well as over-fitting.\n",
    "- `min_data_in_bin`: (default: 3) Minimum number of observations in a bin."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46b52e5-0e47-4a01-b938-60332de94536",
   "metadata": {},
   "source": [
    "### Light GBM Practical Advantages\n",
    "\n",
    "Several practical advantanges of Light GBM are as follows:\n",
    "\n",
    "1. Unlike Scikit-Learn, Light GBM works with Pandas dataframes natively, eliminating the back-forth conversion between Numpy arrays and Pandas dataframes during the modelling & deployment phases.\n",
    "\n",
    "2. With Light GBM, it is straightforward to **deal with the class imbalance issue with the use of a single parameter**. Specifically, Light GBM defines `is_unbalance` which can be set to `True` for classification problems with class imbalance. Setting this parameter to True increases the weight of the positive class inversely proportional to its frequency in the training data.\n",
    "\n",
    "3. Controlling over-fitting is relatively simple with Light GBM with the use of validation data. Light GBM allows for definition of a validation dataset to avoid over-fitting during training. This way, the training can be stopped early when the performance on the validation dataset stops improving.\n",
    "\n",
    "4. We can still fall back on classical Random Forests while still technically using Light GBM by setting the `boosting` parameter to `random_forest`.\n",
    "\n",
    "The way missing values and nominal categorical/ numerical features are handled in Light GBM also eliminates several issues in practice:\n",
    "\n",
    "5. There is no action needed for missing values - other than setting them to `np.nan`.\n",
    "> - Neither for numerical nor categorical features. \n",
    "> - Neither for training data nor test data nor for observations to be predicted during deployment.\n",
    "\n",
    "6. There is no need for encoding of any (nominal) categorical features. \n",
    "> - Neither for training data nor test data nor for observations to be predicted during deployment. \n",
    "> - This eliminates the especially tricky situation in deployment wherein a categorical feature has a missing value or contains a level that was not present in the training dataset.\n",
    "\n",
    "7. There is no need for bundling or eliminating rare levels in a categorical feature.\n",
    ">- Light GBM splits categorical features into just two groups using a specialized algorithm, so neither of the above will be necessary in general.\n",
    "\n",
    "8. For regression problems, Light GBM allows for \"**linear trees**\" at the leaf nodes, which tend to produce better predictions due to the continuous nature of regression problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bffc61-af5f-42f7-98f7-25c8661dc2a8",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Let's split the descriptive features and the target feature into a training set and a test set by a ratio of 70:30. That is, we use 70 % of the data for building **and tuning** a Light GBM classifier and then we evaluate its performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a1f5f3bb-6366-4b89-be00-c2686e5c3bfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data.shape: (39123, 11)\n",
      "Data_train.shape: (27386, 11)\n",
      "Data_test.shape: (11737, 11)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# target is already encoded as 1: high income, 0: low income\n",
    "\n",
    "Data_train, Data_test, t_train, t_test = train_test_split(Data, \n",
    "                                                    target, \n",
    "                                                    test_size = 0.3, \n",
    "                                                    stratify=target,\n",
    "                                                    random_state=999)\n",
    "print(f\"Data.shape: {Data.shape}\")\n",
    "print(f\"Data_train.shape: {Data_train.shape}\")\n",
    "print(f\"Data_test.shape: {Data_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcef2c70-fa7d-4250-b025-483bcc1aa4ed",
   "metadata": {},
   "source": [
    "### Utility Function for Printing Results\n",
    "We define the utility function below to print various metrics for a binary classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ba49bc-873e-46f0-9410-8587609a43d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7df93f3f-0b0f-4f55-bc72-2b7c94a703a4",
   "metadata": {},
   "source": [
    "### Fitting LGBM with Defaults Modified\n",
    "\n",
    "Let's fit a Light GBM with default values changed to fit the problem at hand. Specifically, let's do the following:\n",
    "- `is_unbalance` = True\n",
    "- `max_bin` = 100\n",
    "- `min_data_in_bin` = 10\n",
    "\n",
    "To be clear, these parameters will be **permanently fixed** to the above values and they will not be tuned. This is primarily for illustration purposes to show how certain parameters can be changed from their default values without tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b798eaed-d8b8-4925-b433-9be1a597a0e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8b361ff-383d-410f-a7e7-963f6a6e255a",
   "metadata": {},
   "source": [
    "### (Untuned) Model Performance on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54e934-d7da-40fe-953c-640c6bfaa382",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1a2da58-526c-4627-9153-2626fc168553",
   "metadata": {},
   "source": [
    "### (Untuned) Model Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3137ff-7e1e-4b6a-ba61-c025d536195c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64b5d8cc-2da9-4f81-baf9-e2be1efec72e",
   "metadata": {},
   "source": [
    "## Hyper-parameter Tuning with Optuna <a id='optuna'></a>\n",
    "\n",
    "### Optuna Overview\n",
    "\n",
    "Optuna is a popular framework for hyper-parameter tuning of prediction algorithms. In addition to its good performance & easy parallelization in general, it offers a great deal of flexibility in terms of the search algorithm used for the tuning. Specifically, Optuna allows for the following samplers:\n",
    "- Random search (similar to that of Scikit-Learn)\n",
    "- Grid search (similar to that of Scikit-Learn)\n",
    "- Tree-structured Parzen Estimator (TPE) algorithm: (default sampler) This is a Bayesian independent-sampling based algorithm that uses the ratio of Gaussian Mixture Model (GMM) values for guiding the search.\n",
    "- Covariance Matrix Adaptation Evolution Strategy (CMA-ES) based algorithm: CMA-ES is a stochastic method for real-parameter optimisation of nonlinear, non-convex functions. \n",
    "\n",
    "\n",
    "Some important Optuna terminology is as follows:\n",
    "\n",
    "- `Trial:` A single call of the objective function to be optimised, that is, one objective function measurement.\n",
    "- `Study:` An optimisation session, which is a set of trials.\n",
    "- `Parameter:` A variable whose value is to be optimised.\n",
    "\n",
    "In Optuna, the \"study\" object is used to manage optimisation. Method `create_study()` returns a study object. A study object has useful properties for analyzing the optimization outcome, such as `best_params`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166173c9-abc4-40f6-9cbd-fe447e06486f",
   "metadata": {},
   "source": [
    "### LightGBM Parameters\n",
    "\n",
    "LightGBM [documentation](https://lightgbm.readthedocs.io/en/latest/Parameters.html) lists several dozen parameters, but the more important ones are listed below:\n",
    "\n",
    "- `num_iterations:` (default: 100) Number of trees in the ensemble, i.e., boosting iterations. Internally, LightGBM constructs `num_class * num_iterations` trees for multi-class classification problems.\n",
    "- `learning_rate:` (default: 0.1) Shrinkage rate (weights of the boosting trees). This is the incremental contribution of the subsequent boosting trees in the ensemble for the final prediction.\n",
    "- `num_leaves:` (default: 31) This is the main parameter to control the complexity of the tree model.\n",
    "- `min_child_samples (aliased to min_data_in_leaf):` (default: 20) This is an important parameter to prevent over-fitting in a leaf-wise tree. Its optimal value depends on the number of training samples and num_leaves. \n",
    ">- **NOTE:** Light GBM does not like the name `min_data_in_leaf`, so we will need to use the name `min_child_samples` for this parameter.\n",
    "- `min_gain_to_split:` (default : 0) When adding a new tree node, Light GBM chooses the split point that has the largest gain. Gain is the reduction in training loss that results from adding a split point. By default, Light GBM sets min_gain_to_split to 0.0, which means \"there is no improvement that is too small\". However, in practice, very small improvements in the training loss may not have a meaningful impact on the generalization error of the model. Thus, increasing this parameter may reduce training time with minimal impact on performance.\n",
    "- `max_depth:` (default: -1) Limits the max depth for tree model. This can be used to deal with over-fitting for small datasets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f29a265-d906-43af-b420-2a8fdcb41882",
   "metadata": {},
   "source": [
    "### Parameter Ranges with Optuna \n",
    "\n",
    "In Optuna, a trial is one objective function measurement. This object is passed to an objective function and it provides interfaces to get best parameter values, manage the trial's state, and set/ get user-defined attributes of the trial. Parameters of different kinds can be specified as follows:\n",
    "- `trial.suggest_categorical(name, choices)`\n",
    "- `trial.suggest_int(name, low, high [, step])`\n",
    "- `trial.suggest_float(name, low, high [, step])`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcd4eca1-6c4f-4588-8820-404bf1731288",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Defining the Grid and Objective Function\n",
    "We first define an objective function with **cross-validated** log loss as our performance metric for guiding the tuning process. The `LightGBMPruningCallback()` below from Optuna's integration module identifies unpromising hyper-parameter sets beforehand and reduces the search time significantly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a179705-c4da-466c-8cf5-d6597c2583b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594a3445-2ca3-42fb-8ad6-07e968c63d42",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f13b9e7-a8e6-4862-a561-d9c291b87cd5",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Digression: Grid Search\n",
    "\n",
    "We can also perform a grid search with Optuna. For this, we need to define a \"search space\" that contains the grid values as a list for each hyper-parameter to be tuned, which is illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a4f809-afab-476f-9184-d7d86cfdbd45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c3a9c1b9-2e37-4a6a-b6f1-f2d30059440e",
   "metadata": {},
   "source": [
    "### Random Sampling of the Training Data for Ease of Computation\n",
    "\n",
    "Tuning many hyper-parameters with a lot of data can take a lot of time! In many cases, training with a relatively smaller subset of the original training data can significantly reduce the search time with minimal impact on the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7292213b-9f5c-4927-8a58-1313ebff0838",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54fe89b9-4f93-4150-affc-ad14c8ac0659",
   "metadata": {},
   "source": [
    "### Running the Tuner\n",
    "Let's run the tuner and then have a look at the results. Here, we can set a time limit in seconds, `timeout`, or put a limit on the number of sampling iterations, `n_trials`. We can also invoke parallel processing by setting `n_jobs` to the number of parallel runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5355278-5ecb-4143-b75e-0e4d80893215",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f5c66d9-eb11-4306-93e8-1c8621ef0985",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16588e01-ca53-43eb-9c25-c4d57b69c7dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ebaa215d-4cb1-403a-ba2b-ca833a67108f",
   "metadata": {},
   "source": [
    "We can view the results as a Pandas data frame as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb96e11-9568-4d4b-aded-f2a4ebdfc2d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49be8d4d-7d72-4561-a344-3ae2614d9e20",
   "metadata": {},
   "source": [
    "### Optimisation Hot Start\n",
    "\n",
    "A nice feature of Optuna is that it allows for a \"hot-start\" for the optimisation process. In particular, the study object retains the optimisation history, so when we call the `optimize()` function again, the search continues where it was left off."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f041895-46fd-4eee-9883-c7ee25c1a741",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f370018-c328-40b9-a68e-0e4b3f3bfd79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77e336f7-f42f-4f97-b19d-108109dfb012",
   "metadata": {},
   "source": [
    "### Tuning Commentary\n",
    "\n",
    "Some rough guidelines for tuning are listed below.\n",
    "- Light GBM usually gives very good results with the default parameters already. For this reason, hyper-parameter fine tuning might not bring much to the table, at least not for Light GBM.\n",
    "- Sometimes the Random Sampler can be as competitive as the more fancy samplers such as TPE or CMA-ES.\n",
    "- In case the best value for a hyper-parameter is one of the end points of the defined search range, it may be a good idea to extend the range in that direction to see if even better results might be achieved.\n",
    "- In general, each additional hyper-parameter to be tuned increases the size of the search space significantly, so an iterative approach might make more sense. For instance, we might first search over a few important hyper-parameters, and then fix their values to the best found thus far, and then continue the search over a few other hyper-parameters. Of course, optimality is not guaranteed in this case, but it might help with reducing the search time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81430de-f41e-4531-876c-9b33af0bbe43",
   "metadata": {},
   "source": [
    "## Evaluation & Visualisation of Results <a id='vis'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44172cb7-7d77-4228-a3f6-051ad274e5e4",
   "metadata": {},
   "source": [
    "### Evaluation of the Model with Tuned Parameters\n",
    "\n",
    "Let's train a new model with the best parameters using the train dataset and evaluate it on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72537886-7c35-40ec-8a4a-88c19b8ac836",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "47935d43-37bf-4b61-be16-2fd982db39fd",
   "metadata": {},
   "source": [
    "### Tuned Model Performance on Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d97316b-2cff-490e-a9f9-c9c4903a171c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e1951a9-ee63-4891-bdf3-359f05215743",
   "metadata": {},
   "source": [
    "### Tuned Model Performance on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174610ca-80c6-4e57-9980-5b82d8a1b0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ba32676-a798-43d8-820e-a9697fcbfa6b",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve\n",
    "\n",
    "For our final model, the trade-off between precision and recall can be visualised through a precision-recall curve as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dbf3b69",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145732be-59bf-427c-a44c-c8c434405648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff42d8c-78ec-4d24-bb80-075e0cdaf255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419c1840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67be2c50-f3c3-4e3e-be13-0010140221b9",
   "metadata": {},
   "source": [
    "### Visualisation of Model Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37b194c-b1a9-498c-800b-60a63c49393a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01cfbdda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff3ea9ca-3510-48f8-9753-463dd93d1759",
   "metadata": {},
   "source": [
    "### Visualisation of Tuning Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2c43f-d5af-48d5-ac81-3311483ada49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e812b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab413e06-05cd-4814-8115-4c4b92e203eb",
   "metadata": {},
   "source": [
    "### Visualisation of Hyper-parameter Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d372da64-72e5-4bb7-a3fe-f1c80991e418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0a9ead41",
   "metadata": {},
   "source": [
    "***\n",
    "www.featureranking.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
